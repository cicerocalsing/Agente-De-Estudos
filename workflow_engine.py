from langgraph.graph import StateGraph, END
from typing import TypedDict, Any, List, Dict
from memory_manager import MemoryManager
from llm_service import LLMService
from langdetect import detect
import re

class GraphState(TypedDict):
    pergunta: str
    retriever: Any
    resposta_usuario: str
    memoria: List[Any]
    resposta: str
    avaliacao: str

class WorkflowEngine:
    def __init__(self, memory_manager: MemoryManager, llm_service: LLMService):
        self.memory_manager = memory_manager
        self.llm_service = llm_service

    def route_input(self, state: GraphState) -> str:
        question = state["pergunta"]
        return self.llm_service.classify(question)

    def explanation_node(self, state: GraphState) -> Dict:
        question = state["pergunta"]
        docs = state["retriever"].get_relevant_documents(question)
        context = "\n".join([doc.page_content for doc in docs])
        history = self.memory_manager.search_relevant_memories(question)
        idioma = detect(question)
        if idioma == "en":
            idioma_instrucao = "Please answer in English."
        elif idioma == "pt":
            idioma_instrucao = "Por favor, responda em portuguÃªs."
        else:
            idioma_instrucao = "Please answer in the same language as the question."
        prompt = f"""VocÃª Ã© um assistente educacional inteligente e prestativo da plataforma Cicero Tech AI, criado para ajudar estudantes a entender conteÃºdos acadÃªmicos com clareza, profundidade e foco. Sua missÃ£o Ã© gerar explicaÃ§Ãµes completas e acessÃ­veis, respeitando o nÃ­vel do usuÃ¡rio e sempre utilizando o conteÃºdo presente no material de estudo.

---
ğŸ“š InstruÃ§Ãµes de comportamento:
- Seja claro, direto e didÃ¡tico.  
- Nunca invente informaÃ§Ãµes que nÃ£o estejam no contexto ou histÃ³rico.  
- Se nÃ£o houver informaÃ§Ãµes suficientes, indique isso educadamente e oriente o usuÃ¡rio a revisar o material.  
- Utilize exemplos sempre que possÃ­vel.  
- Evite respostas genÃ©ricas ou vagas.  
- NÃ£o inclua frases como â€œsou um assistenteâ€ ou â€œsou uma IAâ€, apenas entregue o conteÃºdo como um tutor humano faria.  
- Responda a pergunta do usuÃ¡rio no idioma que ela foi feita.
- {idioma_instrucao}

---
ğŸ§  Contexto do conteÃºdo extraÃ­do do PDF:  
{context}

ğŸ“œ HistÃ³rico recente de interaÃ§Ãµes relevantes:  
{history}

â“ Pergunta feita pelo usuÃ¡rio:  
{question}

---
ğŸ” Agora, com base no conteÃºdo e no histÃ³rico, explique a resposta Ã  pergunta acima de forma clara, precisa e amigÃ¡vel. Use exemplos se forem Ãºteis."""

        answer = self.llm_service.call(prompt)
        self.memory_manager.save_conversation(question, answer)
        return {"resposta": answer}

    def question_generation_node(self, state: GraphState) -> Dict:
        docs = state["retriever"].get_relevant_documents("ConteÃºdo Importante!")
        context = "\n".join([doc.page_content for doc in docs])
        history = self.memory_manager.search_relevant_memories("gerar perguntas")
        user_question = state["pergunta"] if "pergunta" in state else ""
        idioma = detect(user_question) if user_question else "pt"
        # Detectar quantidade de perguntas
        match = re.search(r"(\d+)\s+pergunt", user_question.lower())
        n_perguntas = int(match.group(1)) if match else 1
        if idioma == "en":
            idioma_instrucao = f"Please generate {n_perguntas} multiple-choice question(s) and all alternatives in English."
        elif idioma == "pt":
            idioma_instrucao = f"Por favor, gere {n_perguntas} pergunta(s) de mÃºltipla escolha e todas as alternativas em portuguÃªs."
        else:
            idioma_instrucao = f"Please generate {n_perguntas} multiple-choice question(s) and all alternatives in the same language as the user's request."
        exemplo_formato = f"""
Exemplo de formato:
1. [Pergunta 1]
a) ...
b) ...
c) ...
d) ...
Gabarito: Letra X - [resposta correta]

2. [Pergunta 2]
a) ...
b) ...
c) ...
d) ...
Gabarito: Letra X - [resposta correta]
"""
        prompt = f"""VocÃª Ã© um assistente educacional inteligente da plataforma Cicero Tech AI. Sua tarefa Ã© gerar **perguntas de mÃºltipla escolha** sobre conteÃºdos acadÃªmicos com clareza, coerÃªncia e foco pedagÃ³gico. As perguntas devem ajudar o estudante a revisar e testar seu entendimento sobre o conteÃºdo apresentado abaixo.

---
ğŸ¯ InstruÃ§Ãµes de comportamento:
- Gere exatamente {n_perguntas} perguntas, numeradas (1., 2., 3., ...), cada uma com 4 alternativas (a, b, c, d) e o gabarito ao final de cada pergunta.
- Separe cada pergunta com uma linha em branco.
- As perguntas devem ser objetivas, claras e diretamente relacionadas ao conteÃºdo.
- Se o conteÃºdo estiver genÃ©rico, crie perguntas abrangentes e coerentes.
- NÃ£o invente informaÃ§Ãµes fora do conteÃºdo apresentado.
- Use linguagem acessÃ­vel e pedagÃ³gica.
- NÃ£o se identifique como assistente ou IA.
- {idioma_instrucao}
- {exemplo_formato}

---
ğŸ“š Contexto do conteÃºdo extraÃ­do do PDF:  
{context}

ğŸ§  HistÃ³rico recente de interaÃ§Ãµes com o usuÃ¡rio:  
{history}

---
âœï¸ Com base no conteÃºdo acima, crie {n_perguntas} pergunta(s) de mÃºltipla escolha com 4 opÃ§Ãµes cada.  
Inclua o gabarito ao final de cada pergunta no formato:  
**Gabarito: Letra X - [resposta correta]**
"""
        question = self.llm_service.call(prompt)
        self.memory_manager.save_conversation("Me pergunte algo", question)
        return {"resposta": question}

    def build_graph(self):
        graph = StateGraph(GraphState)
        graph.add_node("explicar", self.explanation_node)
        graph.add_node("gerar_pergunta", self.question_generation_node)
        graph.set_conditional_entry_point(self.route_input)
        graph.add_edge("explicar", END)
        graph.add_edge("gerar_pergunta", END)
        return graph.compile() 